{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkpXRXkCxx4a5ra1qJCK70",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BRV12G/Final_year_Project/blob/main/fitness_final_edited_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeBQGAs4xoS1",
        "outputId": "8cf471b3-38e8-490a-e9a4-793b3bd5ca53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded:\n",
            "ID column dropped\n",
            "Numeric Columns after Scaling:\n",
            "Binary Encoded Columns:\n",
            "One-Hot Encoded Columns Added:\n",
            "BMI_Class after Label Encoding:\n",
            "One-Hot Encoding completed for Exercise1 and saved as Exercise1_encoder.pkl.\n",
            "One-Hot Encoding completed for Exercise2 and saved as Exercise2_encoder.pkl.\n",
            "One-Hot Encoding completed for Exercise3 and saved as Exercise3_encoder.pkl.\n",
            "One-Hot Encoded Exercise Columns Added:\n",
            "One-Hot Encoded Equipment Columns Added:\n",
            "Processing d_vegetables...\n",
            "Processing d_juice...\n",
            "Processing d_proteinintake...\n",
            "Text Columns Processed with Multi-Hot Encoding:\n",
            "Preprocessed dataset saved at: /content/preprocessed_dataset.csv\n",
            "Input Features Shape: (14589, 18)\n",
            "Output Features Shape: (14589, 103)\n",
            "Train Shape: (11671, 18) (11671, 103)\n",
            "Validation Shape: (1459, 18) (1459, 103)\n",
            "Test Shape: (1459, 18) (1459, 103)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1588 - mae: 0.2469 - val_loss: 0.0903 - val_mae: 0.1754\n",
            "Epoch 2/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0959 - mae: 0.1875 - val_loss: 0.0794 - val_mae: 0.1563\n",
            "Epoch 3/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0858 - mae: 0.1721 - val_loss: 0.0733 - val_mae: 0.1452\n",
            "Epoch 4/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0794 - mae: 0.1611 - val_loss: 0.0692 - val_mae: 0.1359\n",
            "Epoch 5/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0767 - mae: 0.1559 - val_loss: 0.0675 - val_mae: 0.1321\n",
            "Epoch 6/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0739 - mae: 0.1520 - val_loss: 0.0665 - val_mae: 0.1303\n",
            "Epoch 7/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0734 - mae: 0.1508 - val_loss: 0.0654 - val_mae: 0.1282\n",
            "Epoch 8/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0713 - mae: 0.1480 - val_loss: 0.0656 - val_mae: 0.1318\n",
            "Epoch 9/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0707 - mae: 0.1465 - val_loss: 0.0645 - val_mae: 0.1290\n",
            "Epoch 10/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0699 - mae: 0.1451 - val_loss: 0.0640 - val_mae: 0.1249\n",
            "Epoch 11/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0696 - mae: 0.1449 - val_loss: 0.0636 - val_mae: 0.1258\n",
            "Epoch 12/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0692 - mae: 0.1445 - val_loss: 0.0635 - val_mae: 0.1254\n",
            "Epoch 13/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0689 - mae: 0.1432 - val_loss: 0.0631 - val_mae: 0.1242\n",
            "Epoch 14/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0682 - mae: 0.1429 - val_loss: 0.0634 - val_mae: 0.1275\n",
            "Epoch 15/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0679 - mae: 0.1422 - val_loss: 0.0630 - val_mae: 0.1265\n",
            "Epoch 16/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0674 - mae: 0.1415 - val_loss: 0.0627 - val_mae: 0.1267\n",
            "Epoch 17/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0667 - mae: 0.1405 - val_loss: 0.0626 - val_mae: 0.1236\n",
            "Epoch 18/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0662 - mae: 0.1397 - val_loss: 0.0630 - val_mae: 0.1251\n",
            "Epoch 19/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0668 - mae: 0.1399 - val_loss: 0.0622 - val_mae: 0.1235\n",
            "Epoch 20/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0667 - mae: 0.1397 - val_loss: 0.0625 - val_mae: 0.1233\n",
            "Epoch 21/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0670 - mae: 0.1402 - val_loss: 0.0623 - val_mae: 0.1231\n",
            "Epoch 22/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0669 - mae: 0.1400 - val_loss: 0.0626 - val_mae: 0.1270\n",
            "Epoch 23/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0662 - mae: 0.1389 - val_loss: 0.0622 - val_mae: 0.1240\n",
            "Epoch 24/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0656 - mae: 0.1381 - val_loss: 0.0626 - val_mae: 0.1239\n",
            "Epoch 25/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0657 - mae: 0.1385 - val_loss: 0.0616 - val_mae: 0.1196\n",
            "Epoch 26/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0662 - mae: 0.1391 - val_loss: 0.0615 - val_mae: 0.1188\n",
            "Epoch 27/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0654 - mae: 0.1374 - val_loss: 0.0620 - val_mae: 0.1231\n",
            "Epoch 28/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0656 - mae: 0.1383 - val_loss: 0.0622 - val_mae: 0.1191\n",
            "Epoch 29/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0655 - mae: 0.1377 - val_loss: 0.0620 - val_mae: 0.1242\n",
            "Epoch 30/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0647 - mae: 0.1370 - val_loss: 0.0618 - val_mae: 0.1195\n",
            "Epoch 31/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0653 - mae: 0.1374 - val_loss: 0.0617 - val_mae: 0.1191\n",
            "Epoch 32/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0652 - mae: 0.1373 - val_loss: 0.0617 - val_mae: 0.1215\n",
            "Epoch 33/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0654 - mae: 0.1375 - val_loss: 0.0612 - val_mae: 0.1194\n",
            "Epoch 34/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0654 - mae: 0.1374 - val_loss: 0.0615 - val_mae: 0.1180\n",
            "Epoch 35/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0655 - mae: 0.1379 - val_loss: 0.0613 - val_mae: 0.1190\n",
            "Epoch 36/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0647 - mae: 0.1362 - val_loss: 0.0615 - val_mae: 0.1205\n",
            "Epoch 37/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0648 - mae: 0.1363 - val_loss: 0.0615 - val_mae: 0.1210\n",
            "Epoch 38/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0651 - mae: 0.1372 - val_loss: 0.0620 - val_mae: 0.1245\n",
            "Epoch 39/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0650 - mae: 0.1366 - val_loss: 0.0615 - val_mae: 0.1195\n",
            "Epoch 40/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0644 - mae: 0.1357 - val_loss: 0.0617 - val_mae: 0.1210\n",
            "Epoch 41/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0642 - mae: 0.1356 - val_loss: 0.0613 - val_mae: 0.1194\n",
            "Epoch 42/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0648 - mae: 0.1363 - val_loss: 0.0613 - val_mae: 0.1157\n",
            "Epoch 43/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0650 - mae: 0.1372 - val_loss: 0.0615 - val_mae: 0.1204\n",
            "Epoch 44/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0646 - mae: 0.1368 - val_loss: 0.0613 - val_mae: 0.1195\n",
            "Epoch 45/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0648 - mae: 0.1364 - val_loss: 0.0616 - val_mae: 0.1194\n",
            "Epoch 46/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0637 - mae: 0.1349 - val_loss: 0.0614 - val_mae: 0.1216\n",
            "Epoch 47/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0638 - mae: 0.1359 - val_loss: 0.0612 - val_mae: 0.1202\n",
            "Epoch 48/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0647 - mae: 0.1367 - val_loss: 0.0615 - val_mae: 0.1217\n",
            "Epoch 49/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0647 - mae: 0.1368 - val_loss: 0.0610 - val_mae: 0.1195\n",
            "Epoch 50/50\n",
            "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0643 - mae: 0.1362 - val_loss: 0.0613 - val_mae: 0.1196\n",
            "Mean Train Loss: 0.0691\n",
            "Mean Train MAE: 0.1435\n",
            "Mean Validation Loss: 0.0637\n",
            "Mean Validation MAE: 0.1253\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0595 - mae: 0.1176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Evaluation - Loss: 0.0604, Mean Absolute Error: 0.1182\n",
            "Model has been saved as 'trained_deep_nn_model.h5'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "(1, 103)\n",
            "[[-1.19307387e+00  2.97843981e+00  2.20284760e-01  1.56005785e-01\n",
            "   1.49590030e-01  1.77391171e-01  1.39731795e-01  1.55541852e-01\n",
            "   1.49590552e-01  1.39740050e-01  1.56013221e-01  2.20295817e-01\n",
            "   1.49569586e-01  1.55546799e-01  1.56009391e-01  1.39727056e-01\n",
            "   2.20293984e-01 -8.39573681e-01  6.63359649e-03  1.90656766e-01\n",
            "   1.90625414e-01  1.43911585e-01  1.41592696e-01  1.64635003e-01\n",
            "   1.43926054e-01  1.64645135e-01  1.90647945e-01  1.90654054e-01\n",
            "   1.41576469e-01  3.13526019e-02 -1.49679184e-02 -1.56320632e-04\n",
            "   2.56778635e-02  1.38258487e-02  9.73050594e-01 -5.43811917e-03\n",
            "  -1.01372004e-02 -2.25950778e-03 -7.87167996e-03 -1.01464987e-02\n",
            "  -1.49681717e-02  2.60355622e-02 -1.49676651e-02  1.94626935e-02\n",
            "   9.41257358e-01 -1.58503652e-04  1.79561004e-02  9.41271245e-01\n",
            "  -7.87221640e-03  9.41271245e-01  9.35108960e-01 -7.87261128e-03\n",
            "  -7.87229836e-03 -7.87270814e-03 -2.43968517e-02  4.25855815e-02\n",
            "   9.41259682e-01  1.81198269e-02  2.83123106e-02 -1.49682313e-02\n",
            "  -1.01537704e-02  9.68469977e-01 -1.49662495e-02 -1.49678141e-02\n",
            "   1.94559377e-02 -1.01459920e-02 -1.49675906e-02 -1.56201422e-04\n",
            "   1.94632970e-02 -1.59367919e-04  9.64022398e-01  1.94678083e-02\n",
            "   1.94640625e-02  3.13526802e-02  3.46422195e-02  3.46345305e-02\n",
            "  -1.49682760e-02 -1.49658471e-02 -1.57989562e-04 -1.01346970e-02\n",
            "   2.56781876e-02 -7.87319243e-03 -1.01475418e-02 -1.49673522e-02\n",
            "  -1.57125294e-04  1.94689706e-02 -1.49680674e-02  2.60433704e-02\n",
            "  -1.49678588e-02 -2.43964717e-02  2.56770775e-02  4.25984636e-02\n",
            "  -1.55575573e-04  9.41274166e-01  1.94600467e-02  1.94613338e-02\n",
            "   1.94576848e-02  9.53103065e-01  9.58912671e-01 -7.87240267e-03\n",
            "  -1.56991184e-04  9.58934128e-01  9.35123563e-01]]\n",
            "Decoded BMI Value: 25.362897843783163\n",
            "Decoded Calories Burnt: 562.4354455041631\n",
            "Decoded Water Intake (Litres): 3.6316317333188906\n",
            "Decoded BMI Class: Obuse\n",
            "Decoded Exercises Predictions:\n",
            "['Rowing']\n",
            "Decoded Exercises Predictions:\n",
            "['Rowing', ' Pilates']\n",
            "Decoded Exercises Predictions:\n",
            "['Rowing', ' Pilates', ' Dancing']\n",
            "Decoded Equipment Predictions:\n",
            "['Yoga Mats']\n",
            "Decoded Equipment Predictions:\n",
            "['Yoga Mats', 'Medicine Ball']\n",
            "Decoded Vegetables: ('bell peppers',)\n",
            "Decoded Juice: ('', 'Apple juice', 'Fruit Juice', 'Fruit juice', 'beetroot juice and mango juice')\n",
            "Decoded Protein Intake: ('Beech Nuts', 'Mixed Teff')\n",
            "predictions completed\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ----------------------------------------------------------\n",
        "# Step 1: # Import required libraries\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder ,MultiLabelBinarizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "# Step 1: Load the Dataset\n",
        "# Replace 'your_dataset.csv' with the actual path\n",
        "file_path = '/content/updated_cleaned_diet_split_exercises.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "print(\"Dataset Loaded:\")\n",
        "# print(data.head())\n",
        "#  Drop the ID column (if it exists)\n",
        "if 'ID' in data.columns:\n",
        "    data = data.drop(columns=['ID'])\n",
        "    print(\"ID column dropped\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Step 2: Dataset Preprocessing\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "#  Handling numeric inputs and outputs\n",
        "# ----------------------------------------------------------\n",
        "numeric_cols = ['Age', 'Height', 'Weight', 'Duration_of_Workout']\n",
        "output_numeric_cols = ['BMI', 'Calories_Burnt', 'Water_Intake(Litres)']\n",
        "\n",
        "# Impute Missing Values (mean imputation)\n",
        "data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())\n",
        "data[output_numeric_cols] = data[output_numeric_cols].fillna(data[output_numeric_cols].mean())\n",
        "\n",
        "# Standardize Numeric Columns\n",
        "# scaler = StandardScaler()\n",
        "# Create separate scalers for inputs and outputs\n",
        "input_scaler = StandardScaler()\n",
        "output_scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the input numeric columns and output numeric columns\n",
        "data[numeric_cols] = input_scaler.fit_transform(data[numeric_cols])\n",
        "data[output_numeric_cols] = output_scaler.fit_transform(data[output_numeric_cols])\n",
        "\n",
        "# joblib.dump(scaler, 'scaler.pkl')\n",
        "# Save the scalers separately\n",
        "joblib.dump(input_scaler, 'input_scaler.pkl')\n",
        "joblib.dump(output_scaler, 'output_scaler.pkl')\n",
        "\n",
        "print(\"Numeric Columns after Scaling:\")\n",
        "# print(data[numeric_cols + output_numeric_cols].head())\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Handling Binary categorical columns using label encoding\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "# Binary Encoding for Binary Categorical Columns\n",
        "binary_cols = ['Gender', 'Hypertension', 'Diabetes']\n",
        "label_encoders = {}\n",
        "for col in binary_cols:\n",
        "    le = LabelEncoder()\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "    label_encoders[col] = le\n",
        "    joblib.dump(le, f'{col}_encoder.pkl')\n",
        "print(\"Binary Encoded Columns:\")\n",
        "# print(data[binary_cols].head())\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        " # One hot encoding for categorical columns  fitness goal and fitness type\n",
        "#------------------------------------------------------------------------------\n",
        "fitness_goal_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
        "fitness_type_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
        "# Fit and transform 'Fitness_Goal'\n",
        "fitness_goal_encoded = fitness_goal_encoder.fit_transform(data[['Fitness_Goal']])\n",
        "fitness_goal_columns = fitness_goal_encoder.get_feature_names_out(['Fitness_Goal'])\n",
        "# Create a DataFrame for 'Fitness_Goal' encoded data\n",
        "fitness_goal_df = pd.DataFrame(fitness_goal_encoded, columns=fitness_goal_columns, index=data.index)\n",
        "# Fit and transform 'Fitness_Type'\n",
        "fitness_type_encoded = fitness_type_encoder.fit_transform(data[['Fitness_Type']])\n",
        "fitness_type_columns = fitness_type_encoder.get_feature_names_out(['Fitness_Type'])\n",
        "# Create a DataFrame for 'Fitness_Type' encoded data\n",
        "fitness_type_df = pd.DataFrame(fitness_type_encoded, columns=fitness_type_columns, index=data.index)\n",
        "# Concatenate the original data with the encoded columns and drop the original columns\n",
        "data = pd.concat([data, fitness_goal_df, fitness_type_df], axis=1)\n",
        "data = data.drop(columns=['Fitness_Goal', 'Fitness_Type'])\n",
        "# Save the OneHotEncoder objects for reuse\n",
        "joblib.dump(fitness_goal_encoder, 'fitness_goal_encoder.pkl')\n",
        "joblib.dump(fitness_type_encoder, 'fitness_type_encoder.pkl')\n",
        "# Save the column names for each encoder\n",
        "joblib.dump(list(fitness_goal_columns), 'fitness_goal_columns.pkl')\n",
        "joblib.dump(list(fitness_type_columns), 'fitness_type_columns.pkl')\n",
        "print(\"One-Hot Encoded Columns Added:\")\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        " #label encoding for bmi class since they are ordinal\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "# Label Encoding for BMI_Class\n",
        "bmi_class_encoder = LabelEncoder()\n",
        "data['BMI_Class'] = bmi_class_encoder.fit_transform(data['BMI_Class'])\n",
        "# Save the LabelEncoder for future use\n",
        "joblib.dump(bmi_class_encoder, 'bmi_class_encoder.pkl')\n",
        "print(\"BMI_Class after Label Encoding:\")\n",
        "# print(data['BMI_Class'].head())\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# one-hot encoding for exercise columns\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "exercise_cols = ['Exercise1', 'Exercise2', 'Exercise3']\n",
        "one_hot_encoders = {}  # Dictionary to store OneHotEncoders for each column\n",
        "# Loop through each exercise column and apply OneHotEncoder\n",
        "for col in exercise_cols:\n",
        "    # Initialize OneHotEncoder\n",
        "    encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
        "    # Fit and transform the column\n",
        "    encoded_data = encoder.fit_transform(data[[col]])\n",
        "    # Add the one-hot encoded columns to the original DataFrame\n",
        "    encoded_df = pd.DataFrame(\n",
        "        encoded_data,\n",
        "        columns=[f\"{col}_{cat}\" for cat in encoder.categories_[0][1:]]  # Exclude the first category due to 'drop=\"first\"'\n",
        "    )\n",
        "    data = pd.concat([data, encoded_df], axis=1)\n",
        "    # Save the encoder for future use\n",
        "    one_hot_encoders[col] = encoder\n",
        "    joblib.dump(encoder, f\"{col}_encoder.pkl\")\n",
        "    print(f\"One-Hot Encoding completed for {col} and saved as {col}_encoder.pkl.\")\n",
        "# Drop the original exercise columns\n",
        "data = data.drop(columns=exercise_cols)\n",
        "print(\"One-Hot Encoded Exercise Columns Added:\")\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# One-Hot Encoding for Equipment Columns\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "# Define the equipment columns\n",
        "equipment_cols = ['Equipment1', 'Equipment2']\n",
        "# Initialize OneHotEncoders for each equipment column\n",
        "equipment1_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
        "equipment2_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
        "# Apply OneHotEncoding for 'Equipment1'\n",
        "equipment1_encoded = equipment1_encoder.fit_transform(data[['Equipment1']])\n",
        "equipment1_columns = equipment1_encoder.get_feature_names_out(['Equipment1'])\n",
        "# Add the encoded columns back to the DataFrame\n",
        "data_equipment1 = pd.DataFrame(equipment1_encoded, columns=equipment1_columns, index=data.index)\n",
        "data = pd.concat([data, data_equipment1], axis=1)\n",
        "# Apply OneHotEncoding for 'Equipment2'\n",
        "equipment2_encoded = equipment2_encoder.fit_transform(data[['Equipment2']])\n",
        "equipment2_columns = equipment2_encoder.get_feature_names_out(['Equipment2'])\n",
        "# Add the encoded columns back to the DataFrame\n",
        "data_equipment2 = pd.DataFrame(equipment2_encoded, columns=equipment2_columns, index=data.index)\n",
        "data = pd.concat([data, data_equipment2], axis=1)\n",
        "# Drop the original columns\n",
        "data = data.drop(columns=equipment_cols)\n",
        "# Save the encoders and the column names for future use\n",
        "joblib.dump(equipment1_encoder, 'equipment1_encoder.pkl')\n",
        "joblib.dump(equipment2_encoder, 'equipment2_encoder.pkl')\n",
        "joblib.dump(list(equipment1_columns), 'equipment1_columns.pkl')\n",
        "joblib.dump(list(equipment2_columns), 'equipment2_columns.pkl')\n",
        "\n",
        "print(\"One-Hot Encoded Equipment Columns Added:\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# multi-hot encoding  for Text Columns\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "# Step 1: Initialize MultiLabelBinarizer\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "# Step 2: Process each text column\n",
        "mlb_dict = {}  # Dictionary to store MultiLabelBinarizer for each text column\n",
        "text_columns = ['d_vegetables', 'd_juice', 'd_proteinintake']\n",
        "for col in text_columns:\n",
        "    print(f\"Processing {col}...\")\n",
        "\n",
        "    # Split by commas and strip spaces\n",
        "    split_data = data[col].fillna(\"\").str.split(\", \").apply(lambda x: [item.strip() for item in x])\n",
        "\n",
        "    # MultiHot encoding using MultiLabelBinarizer\n",
        "    multi_hot_encoded = mlb.fit_transform(split_data)\n",
        "\n",
        "    # Save the fitted MultiLabelBinarizer for reuse\n",
        "    mlb_dict[col] = mlb\n",
        "    joblib.dump(mlb, f'{col}_mlb.pkl')  # Save each MultiLabelBinarizer separately\n",
        "\n",
        "    # Create a DataFrame with multi-hot encoded features\n",
        "    multi_hot_df = pd.DataFrame(multi_hot_encoded, columns=[f\"{col}_{item}\" for item in mlb.classes_])\n",
        "\n",
        "    # Merge the multi-hot encoded columns with the main DataFrame\n",
        "    data = pd.concat([data, multi_hot_df], axis=1)\n",
        "\n",
        "# Step 3: Drop the original text columns\n",
        "data = data.drop(columns=text_columns)\n",
        "\n",
        "# Display the processed DataFrame\n",
        "print(\"Text Columns Processed with Multi-Hot Encoding:\")\n",
        "# print(data.head())\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------------\n",
        "#  Save the Preprocessed Dataset\n",
        "#-----------------------------------------------------------------------------\n",
        "preprocessed_file_path = '/content/preprocessed_dataset.csv'\n",
        "data.to_csv(preprocessed_file_path, index=False)\n",
        "\n",
        "print(f\"Preprocessed dataset saved at: {preprocessed_file_path}\")\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "#--------------------------------------------------------------------------------\n",
        "#--------------------------------------------------------------------------------\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Step 3: Splitting Data into Inputs (X) and Outputs (y)\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "# Define Input Columns (features)\n",
        "input_features = ['Gender', 'Age', 'Height', 'Weight', 'Hypertension', 'Diabetes', 'Duration_of_Workout'] + \\\n",
        "                 list(data.filter(like='Fitness_Goal_')) + \\\n",
        "                 list(data.filter(like='Fitness_Type_'))\n",
        "\n",
        "\n",
        "\n",
        "output_features = ['BMI', 'BMI_Class'] + \\\n",
        "                 list(data.filter(like='Exercise1_')) + \\\n",
        "                 list(data.filter(like='Exercise2_')) + \\\n",
        "                 list(data.filter(like='Exercise3_')) + ['Calories_Burnt', 'Water_Intake(Litres)'] + \\\n",
        "                 list(data.filter(like='Equipment1_')) + \\\n",
        "                 list(data.filter(like='Equipment2_')) + \\\n",
        "                 list(data.filter(like='d_'))\n",
        "\n",
        "X = data[input_features]\n",
        "y = data[output_features]\n",
        "\n",
        "print(\"Input Features Shape:\", X.shape)\n",
        "print(\"Output Features Shape:\", y.shape)\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Step 4: Splitting Data into Train, Validation, and Test Sets\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(\"Train Shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Validation Shape:\", X_val.shape, y_val.shape)\n",
        "print(\"Test Shape:\", X_test.shape, y_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# ----------------------------------------------------------\n",
        "# Step 5: Training , building and compiling the model\n",
        "# ----------------------------------------------------------\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "# Import necessary libraries for deep learning\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "# Step 1: Build the Deep Learning Model\n",
        "model = Sequential()\n",
        "\n",
        "# Input Layer\n",
        "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "\n",
        "# Hidden Layers\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.3))  # Dropout layer for regularization\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(y_train.shape[1], activation='linear'))  # Linear activation for regression tasks\n",
        "\n",
        "# Step 2: Compile the Model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Step 3: Train the Model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# # Step 6: Evaluate the Model on training , testing and validation sets\n",
        "#-------------------------------------------------------------------------------\n",
        "#train data evaluation\n",
        "mean_train_loss = sum(history.history['loss']) / len(history.history['loss'])\n",
        "mean_train_mae = sum(history.history['mae']) / len(history.history['mae'])\n",
        "print(f\"Mean Train Loss: {mean_train_loss:.4f}\")\n",
        "print(f\"Mean Train MAE: {mean_train_mae:.4f}\")\n",
        "\n",
        "#validation data evaluation\n",
        "mean_val_loss = sum(history.history['val_loss']) / len(history.history['val_loss'])\n",
        "mean_val_mae = sum(history.history['val_mae']) / len(history.history['val_mae'])\n",
        "print(f\"Mean Validation Loss: {mean_val_loss:.4f}\")\n",
        "print(f\"Mean Validation MAE: {mean_val_mae:.4f}\")\n",
        "\n",
        "#test data evaluation\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f\"Model Evaluation - Loss: {loss:.4f}, Mean Absolute Error: {mae:.4f}\")\n",
        "\n",
        "\n",
        "model.save('trained_deep_nn_model.h5')\n",
        "\n",
        "print(\"Model has been saved as 'trained_deep_nn_model.h5'\")\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# step 7: testing the model on user inputs\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "\n",
        "# Load the encoders and scalers saved during training\n",
        "gender_encoder = joblib.load('/content/Gender_encoder.pkl')\n",
        "hypertension_encoder = joblib.load('/content/Hypertension_encoder.pkl')\n",
        "diabetes_encoder = joblib.load('/content/Diabetes_encoder.pkl')\n",
        "\n",
        "fitness_goal_encoder = joblib.load('/content/fitness_goal_encoder.pkl')\n",
        "fitness_type_encoder = joblib.load('/content/fitness_type_encoder.pkl')\n",
        "\n",
        "input_scaler = joblib.load('/content/input_scaler.pkl')\n",
        "output_scaler = joblib.load('/content/output_scaler.pkl')\n",
        "bmi_class_encoder = joblib.load('/content/bmi_class_encoder.pkl')\n",
        "\n",
        "exercise_encoders = {col: joblib.load(f\"/content/{col}_encoder.pkl\") for col in ['Exercise1', 'Exercise2', 'Exercise3']}\n",
        "equipment_encoders = {\n",
        "    'Equipment1': joblib.load('equipment1_encoder.pkl'),\n",
        "    'Equipment2': joblib.load('equipment2_encoder.pkl')\n",
        "}\n",
        "\n",
        "# Load the pre-trained MultiLabelBinarizer objects\n",
        "mlb_vegetables = joblib.load('/content/d_vegetables_mlb.pkl')\n",
        "mlb_juice = joblib.load('/content/d_juice_mlb.pkl')\n",
        "mlb_proteinintake = joblib.load('/content/d_proteinintake_mlb.pkl')\n",
        "\n",
        "mlb_dict = {col: joblib.load(f'/content/{col}_mlb.pkl') for col in ['d_vegetables', 'd_juice', 'd_proteinintake']}\n",
        "\n",
        "# Load the trained deep neural network model\n",
        "model = load_model('/content/trained_deep_nn_model.h5')\n",
        "\n",
        "# Example user input\n",
        "user_input = {\n",
        "    'Gender': ['Female'],\n",
        "    'Age': [18],\n",
        "    'Height': [168],\n",
        "    'Weight': [47.5],\n",
        "    'Hypertension': ['Yes'],\n",
        "    'Diabetes': ['No'],\n",
        "    'Duration_of_Workout': [35],\n",
        "    'Fitness_Goal': ['Weight Gain'],\n",
        "    'Fitness_Type': ['Cardio'],\n",
        "}\n",
        "\n",
        "# Step 1: Preprocess the input data\n",
        "user_input_df = pd.DataFrame(user_input)\n",
        "\n",
        "# Apply label encoding\n",
        "user_input_df['Gender'] = gender_encoder.transform(user_input_df['Gender'])\n",
        "user_input_df['Hypertension'] = hypertension_encoder.transform(user_input_df['Hypertension'])\n",
        "user_input_df['Diabetes'] = diabetes_encoder.transform(user_input_df['Diabetes'])\n",
        "\n",
        "# Apply one-hot encoding\n",
        "fitness_goal_encoded = fitness_goal_encoder.transform(user_input_df[['Fitness_Goal']])\n",
        "fitness_type_encoded = fitness_type_encoder.transform(user_input_df[['Fitness_Type']])\n",
        "\n",
        "fitness_goal_df = pd.DataFrame(fitness_goal_encoded, columns=fitness_goal_encoder.get_feature_names_out())\n",
        "fitness_type_df = pd.DataFrame(fitness_type_encoded, columns=fitness_type_encoder.get_feature_names_out())\n",
        "\n",
        "# Merge encoded columns and drop the originals\n",
        "user_input_df = pd.concat([user_input_df, fitness_goal_df, fitness_type_df], axis=1)\n",
        "user_input_df = user_input_df.drop(columns=['Fitness_Goal', 'Fitness_Type'])\n",
        "\n",
        "# Scale numeric columns\n",
        "numeric_columns = ['Age', 'Height', 'Weight', 'Duration_of_Workout']\n",
        "user_input_df[numeric_columns] = input_scaler.transform(user_input_df[numeric_columns])\n",
        "\n",
        "# Step 2: Make predictions\n",
        "predictions = model.predict(user_input_df)\n",
        "\n",
        "print(predictions.shape)\n",
        "# print(predictions[:5, :])\n",
        "print(predictions)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the columns corresponding to the predictions\n",
        "input_columns = ['Gender', 'Age', 'Height', 'Weight', 'Hypertension', 'Diabetes', 'Duration_of_Workout', 'Fitness_Goal', 'Fitness_Type']\n",
        "output_columns = ['BMI', 'BMI_Class', 'Exercise1', 'Exercise2', 'Exercise3', 'Calories_Burnt', 'Water_Intake(Litres)', 'Equipment1', 'Equipment2',\n",
        "                  'd_vegetables', 'd_juice', 'd_proteinintake']\n",
        "\n",
        "# Assuming predictions is the array of predictions\n",
        "predictions = predictions[0]  # Remove the batch dimension, shape becomes (103,)\n",
        "\n",
        "# Separate predictions into the input and output parts\n",
        "input_predictions = predictions[:len(input_columns)]  # First part corresponds to inputs\n",
        "output_predictions = predictions[len(input_columns):]  # Remaining part corresponds to outputs\n",
        "\n",
        "\n",
        "# Assuming positions in output_scaler: BMI = 0, Calories_Burnt = 1, Water_Intake(Litres) = 2\n",
        "\n",
        "output_scaler_columns = ['BMI', 'Calories_Burnt', 'Water_Intake(Litres)']\n",
        "\n",
        "# Extract the specific positions for each feature\n",
        "bmi_value = output_scaler.inverse_transform([[output_predictions[0], 0, 0]])[0][0]  # Only BMI value\n",
        "calories_burnt = output_scaler.inverse_transform([[0, output_predictions[4], 0]])[0][1]  # Only Calories_Burnt value\n",
        "water_intake = output_scaler.inverse_transform([[0, 0, output_predictions[5]]])[0][2]  # Only Water_Intake(Litres)\n",
        "bmi_class_value = bmi_class_encoder.inverse_transform([np.argmax(output_predictions[1:4])])[0]\n",
        "\n",
        "# Print the decoded values\n",
        "print(\"Decoded BMI Value:\", bmi_value)\n",
        "print(\"Decoded Calories Burnt:\", calories_burnt)\n",
        "print(\"Decoded Water Intake (Litres):\", water_intake)\n",
        "print(\"Decoded BMI Class:\", bmi_class_value)\n",
        "\n",
        "# Process One-Hot Encoded Outputs (like Exercise1, Exercise2, Exercise3, Equipment1, Equipment2)\n",
        "exercise_predictions = output_predictions[:5 * len(['Exercise1', 'Exercise2', 'Exercise3'])]  # Assuming 3 classes for each exercise\n",
        "equipment_predictions = output_predictions[-len(['Equipment1', 'Equipment2']) * 5:]  # Assuming Equipment1, Equipment2 are one-hot encoded\n",
        "\n",
        "# Inverse transform categorical predictions\n",
        "decoded_exercises = []\n",
        "for i, col in enumerate(['Exercise1', 'Exercise2', 'Exercise3']):\n",
        "    start_idx = i * 5\n",
        "    end_idx = (i + 1) * 5\n",
        "    exercise_probs = exercise_predictions[start_idx:end_idx]\n",
        "    # Get the predicted class index (index with max probability)\n",
        "    predicted_exercise_index = np.argmax(exercise_probs)\n",
        "    # Create a one-hot vector with the max index set to 1\n",
        "    one_hot_vector = [0] * 5  # Assuming 6 classes for exercises\n",
        "    one_hot_vector[predicted_exercise_index] = 1\n",
        "    # Inverse transform to get the actual exercise class\n",
        "    predicted_exercise = exercise_encoders[col].inverse_transform([one_hot_vector])\n",
        "    decoded_exercises.append(predicted_exercise[0][0])\n",
        "    print(\"Decoded Exercises Predictions:\")\n",
        "    print(decoded_exercises)\n",
        "\n",
        "\n",
        "decoded_equipment = []\n",
        "for i, col in enumerate(['Equipment1', 'Equipment2']):\n",
        "    start_idx = i * 5\n",
        "    end_idx = (i + 1) * 5\n",
        "    equipment_probs = equipment_predictions[start_idx:end_idx]\n",
        "    # Get the predicted class index (index with max probability)\n",
        "    predicted_equipment_index = np.argmax(equipment_probs)\n",
        "    # Create a one-hot vector with the max index set to 1\n",
        "    one_hot_vector = [0] * 5  # Assuming 6 classes for equipment\n",
        "    one_hot_vector[predicted_equipment_index] = 1\n",
        "    # Inverse transform to get the actual equipment class\n",
        "    predicted_equipment = equipment_encoders[col].inverse_transform([one_hot_vector])\n",
        "    decoded_equipment.append(predicted_equipment[0][0])\n",
        "    print(\"Decoded Equipment Predictions:\")\n",
        "    print(decoded_equipment)\n",
        "\n",
        "\n",
        "# Extract the multi-hot encoded values for each category\n",
        "multi_hot_vegetables = output_predictions[6:6 + 29]  # 29 unique vegetables\n",
        "multi_hot_juice = output_predictions[6 + 29:6 + 29 + 15]  # 15 unique juice items\n",
        "multi_hot_protein = output_predictions[6 + 29 + 15:6 + 29 + 15 + 30]  # 30 unique protein intake items\n",
        "\n",
        "# Threshold the multi-hot encoded outputs (e.g., using 0.5 as the threshold)\n",
        "threshold_vegetables = (multi_hot_vegetables > 0.5).astype(int)\n",
        "threshold_juice = (multi_hot_juice > 0.5).astype(int)\n",
        "threshold_protein = (multi_hot_protein > 0.5).astype(int)\n",
        "\n",
        "# Inverse transform the thresholded multi-hot values\n",
        "decoded_vegetables = mlb_vegetables.inverse_transform(np.array([threshold_vegetables]))[0]\n",
        "decoded_juice = mlb_juice.inverse_transform(np.array([threshold_juice]))[0]\n",
        "decoded_proteinintake = mlb_proteinintake.inverse_transform(np.array([threshold_protein]))[0]\n",
        "\n",
        "\n",
        "# Now you have decoded results as lists of categories\n",
        "print(\"Decoded Vegetables:\", decoded_vegetables)\n",
        "print(\"Decoded Juice:\", decoded_juice)\n",
        "print(\"Decoded Protein Intake:\", decoded_proteinintake)\n",
        "\n",
        "print (\"predictions completed\")\n",
        "\n"
      ]
    }
  ]
}